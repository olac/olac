#summary 2.1 Crosswalking

[Metadata 1.1 Metadata] | [Participation 1.2 Participation] | [Curation 1.3 Curation] | [Crosswalking 2.1 Crosswalking] | [Mining 2.2 Mining] | [Search 2.3 Search]

===2.1 OLAC is indexing all major repository and library holdings relevant to language documentation and description by crosswalking and enriching existing catalog records.===

2.1a _Language identification:_ Develop an automated procedure for finding language identification in a catalog record and translating it to an appropriate ISO 639-3 code.

  * *Deliverable:* A trained classifier that can classify a metadata record as to ISO 639-3 codes that appear to be relevant to it. See [LanguageIdentification implementation notes].<br>
  * *Status:* A rule-based classifier has been implemented. More testing needs to be done to tune the parameters for optimal performance.<br>
  * *Deliverable:* A data set for training the language classifier, containing alternate names, dialect names, geographic information, and language classification.<br>
  * *Status:* A set of data files for training the classifier has been developed. More testing and evaluation needs to be done to refine the training data for optimal classifier performance.<br>

2.1b _Data type identification:_ Develop an automated procedure for finding linguistic data type information in a catalog record and translating it to an appropriate OLAC linguistic data type code.

  * *Deliverable:* A binary classifier that can classify a metadata record as to whether or not it is a language resource.<br>
  * *Status:* Training data has been created based on titles and subject headings of 5 million Library of Congress catalog records (using call numbers to divide them into language resources versus non-language resources). This data (along with the MALLET package) has been used to train a maximum entropy classifier.<br>
  * *Deliverable:* A multi-way classifier that can classify a metadata record known to be a language resource as to the linguistic data types that appear to be relevant to it.<br>
  * *Status:* Training data has been created based on titles, descriptions, and subject headings of 80,000 Library of Congress catalog records of known language resources(using subject headings to sort them into six language resource types). This data (along with the MALLET package) has been used to train a maximum entropy classifier.<br>

2.1c _MARC Crosswalk:_ Develop a crosswalk that transforms a set of MARC catalog records for language resources into an OLAC static repository.

  * *Deliverable:* A processor (see [http://code.google.com/p/olac/wiki/MARC specification]) that transforms a set of MARC catalog records supplied by a participating archive to OLAC format, and which uses the classifiers developed in 2.1a and 2.1b to add language codes and linguistic data types when they are not available in the original data.<br>
  * *Status:* A rule-based crosswalk has been implemented in Python and XSLT; see [http://www.sil.org/~simonsg/reprint/jcdl2009.pdf abstract] and [http://www.sil.org/~simonsg/poster/MARC-to-OLAC.pdf poster]. However, the classifiers developed in 2.1a and 2.1b have not yet been integrated.<br>
  * *Deliverable:* A static repository for language resources in the GIAL library built from the MARC dataset they have provided.<br>
  * *Status:* The rule-based crosswalk gave satisfactory results for this collection and it is now operational; see http://www.language-archives.org/archive/gial.edu.<br>
  * *Deliverable:* A static repository for language resources in the National Anthropological Archive built from the MARC dataset they have provided.<br>
  * *Status:* The results achieved without using the classifiers developed in 2.1a and 2.1b were disappointing in terms of percentage of records with precise language and resource type identification. Completion is awaiting integration of the automated classifiers into the crosswalk processor.<br>
  * *Deliverable:* All code in the Subversion repository with an OLAC Note documenting the system so that participating institutions would be able to run it themselves.<br>
  * *Status:* The code is in the repository at http://code.google.com/p/olac/source/browse/#svn/src/marc-crosswalk. The document is not started.<br>

2.1d _OAI Crosswalk:_ Develop an OLAC data provider that is a crosswalk for language resource records discovered in the leading OAI aggregator.

  * *Deliverable:* A gateway service that harvests all registered OAI data providers and uses the classifiers developed in 2.1a and 2.1b to identify language resources and crosswalk them to OLAC with metadata enriched to add language codes and linguistic data types.<br>
  * *Status:* A version of the OLAC harvester has been created to harvest oai_dc, and about 600 repositories have been harvested. Work is in progress to integrate the classifiers developed in 2.1a and 2.1b.<br>

2.1e _Z39.50 Crosswalk:_ Develop an OLAC data provider that is a crosswalk for language resource records discovered in leading Z39.50 library gateways.

  * *Deliverable:* As an alternate strategy we are going to try to give access to library collections through WorldCat..<br>
  * *Status:* Proof of concept implementation has been done, but cannot go operational until we have clearance from OCLC to go live.<br>
  * 
NB. there is a duplication issue if we actually harvest from multiple libraries; pick a small number of libraries with SRU support. Separate task: can we harvest a listing of books in print? http://code.google.com/p/olac/issues/detail?id=74