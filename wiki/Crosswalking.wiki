#summary 2.1 Crosswalking

[Metadata 1.1 Metadata] | [Participation 1.2 Participation] | [Curation 1.3 Curation] | [Crosswalking 2.1 Crosswalking] | [Mining 2.2 Mining] | [Search 2.3 Search]

===Outcome 2.1: OLAC is indexing all major repository and library holdings relevant to language documentation and description by crosswalking and enriching existing catalog records.===

2.1a _Language identification:_ Develop an automated procedure for finding language names in a catalog record and translating them to an appropriate ISO 639-3 code.

  * *Deliverable:* A trained classifier that can classify a metadata record as to ISO 639-3 codes that appear to be relevant to it. See [LanguageIdentification implementation notes].<br>
  * *Status:* A rule-based classifier has been implemented.<br>
  * *Deliverable:* A data set for training the language classifier, containing alternate names, dialect names, geographic information, and language classification.<br>
  * *Status:* A set of data files for training the classifier has been developed. Results are reported in a [http://www.sil.org/~simonsg/presentation/DH%202011.pdf conference presentation].<br>

2.1b _Data type identification:_ Develop an automated procedure for finding linguistic data type information in a catalog record and translating it to an appropriate OLAC linguistic data type code.

  * *Deliverable:* A binary classifier that can classify a metadata record as to whether or not it is a language resource.<br>
  * *Status:* Training data has been created based on titles and subject headings of 5 million Library of Congress catalog records (using call numbers to divide them into language resources versus non-language resources). This data has been used to train a maximum entropy classifier (using the MALLET package). Results are reported in a [http://www.sil.org/~simonsg/presentation/DH%202011.pdf conference presentation].<br>
  * *Deliverable:* A multi-way classifier that can classify a metadata record known to be a language resource as to the linguistic data types that appear to be relevant to it.<br>
  * *Status:* Training data has been created based on titles, descriptions, and subject headings of 80,000 Library of Congress catalog records of known language resources(using subject headings to sort them into six language resource types). This data has been used to train a maximum entropy classifier (using the MALLET package). Evaluation has shown that the results achieved by this classifier are not yet satisfactory.<br>

2.1c _MARC Crosswalk:_ Develop a crosswalk that transforms a set of MARC catalog records for language resources into an OLAC static repository.

  * *Deliverable:* A processor (see [http://code.google.com/p/olac/wiki/MARC specification]) that transforms a set of MARC catalog records supplied by a participating archive to OLAC format, and which uses the classifiers developed in 2.1a and 2.1b to add language codes and linguistic data types when they are not available in the original data.<br>
  * *Status:* A rule-based crosswalk has been implemented in Python and XSLT; see [http://www.sil.org/~simonsg/reprint/jcdl2009.pdf abstract] and [http://www.sil.org/~simonsg/poster/MARC-to-OLAC.pdf poster]. The classifiers developed in 2.1a and 2.1b have been integrated.<br>
  * *Deliverable:* A static repository for language resources in the GIAL library built from the MARC dataset they have provided.<br>
  * *Status:* The rule-based crosswalk gave satisfactory results for this collection and it is now operational; see http://www.language-archives.org/archive/gial.edu.<br>
  * *Deliverable:* A static repository for language resources in the National Anthropological Archive built from the MARC dataset they have provided.<br>
  * *Status:* The results achieved initially were disappointing in that only 59% of the records had specific language identification in subject headings. It remains to rerun the crosswalk using the newly integrated language identification classifier over titles and evaluate again..<br>
  * *Deliverable:* All crosswalk code is in the Subversion repository with an OLAC Note documenting the system so that participating institutions would be able to run it themselves.<br>
  * *Status:* The code is in the repository at http://code.google.com/p/olac/source/browse/#svn/src/marc-crosswalk. The document remains to be completed.<br>

2.1d _OAI Crosswalk:_ Develop an OLAC data provider that is a crosswalk for language resource records discovered in all known OAI repositories (over 2400 are registered at http://gita.grainger.uiuc.edu/registry/searchform.asp).

  * *Deliverable:* A gateway service that harvests all registered OAI data providers and uses the classifiers developed in 2.1a and 2.1b to identify language resources and crosswalk them to OLAC with metadata enriched to add language codes and linguistic data types.<br>
  * *Status:* A version of the OLAC harvester has been created to harvest oai_dc, and 5 million records have been harvested from about 450 repositories. Approximately 22,000 presumed language resources have been identified as reported in a [http://www.sil.org/~simonsg/presentation/DH%202011.pdf conference presentation].<br>

2.1e _Z39.50 Crosswalk:_ Develop an OLAC data provider that is a crosswalk for language resource records discovered in leading Z39.50 library gateways.

  * *Deliverable:* As an alternate strategy we are going to try to give access to library collections through WorldCat..<br>
  * *Status:* A proof of concept implementation has been done, because OCLC will not permit WorldCat to be used for discovery purposes.<br>
  * 
NB. there is a duplication issue if we actually harvest from multiple libraries; pick a small number of libraries with SRU support. Separate task: can we harvest a listing of books in print? http://code.google.com/p/olac/issues/detail?id=74