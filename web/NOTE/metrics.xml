<?xml version="1.0"?>
<?xml-stylesheet href="../OLAC_doc.xsl" type="text/xsl"?>
<!DOCTYPE OLAC_doc SYSTEM "../OLAC_doc.dtd">
<OLAC_doc>
   <header>
      <status code="draft" type="informational"/>
      <title>OLAC metadata quality metrics</title>
      <baseName>metrics</baseName>
      <issued>20071023</issued>
      <previousIssued/>
      <abstract>
         <p>Explains the metrics that are implemented on the OLAC web site for evaluating the quality of metadata
            records.</p>
      </abstract>
      <editors>Gary Simons, SIL International (<url>mailto:gary_simons@sil.org</url>)</editors>
      <copyright>2007 Gary Simons (SIL International)</copyright>
      <changes>
      </changes>
   </header>
   <body>
      <section>
         <heading>Introduction</heading>
         <p>The vision of OLAC is that "any user on the Internet should be able to go to a
            single gateway to find all the language resources available at all
            participating institutions" (see vision statement in 
            <cit>OLAC-Process</cit>). The ability of a user to discover any relevant
            language resource is dependent on the quality of the metadata that describes
            it. Ensuring quality through peer review is a core value that OLAC employs to
            achieve its vision. "OLAC also conducts automated review based on peer
            consensus regarding best practice" (see core value statements in 
            <cit>OLAC-Process</cit>). </p>
         <p>This note explains the automated system that is implemented on the OLAC web site for evaluating the quality of metadata
            records. The peer consensus regarding best practice is expressed in
            <cit>OLAC-BPR</cit> and further elucidated in <cit>OLAC-Usage</cit>. </p>

      </section>
    <section>
         <heading>The quality score</heading>
       <p>Many of the best practice recommendations for resource description 
          cannot be automatically checked for
          conformance; however, there are many that can be. As an aid in creating
          descriptive metadata that meets the latter set of recommendations an automated
          quality score has been implemented. Each metadata record receives a score in the
       range of 0 to 10 based on the presence or absence of recommended practices. In some
       contexts the score is reported as a number in this range; in others it is
       summarized graphically as a rating of 1 to 5 stars.  That is, any score above 9 is
       reported as 5 stars, scors in the range of 7 to 9 are reported as 4 stars, and so
       on.</p>
       <p>The practices in focus are ones that contribute to resource discovery. The score has
          two major parts: 75% is based on the metadata elements that are
          present and 25% is based on the use of encoding schemes.  The elements provide
          the breadth and depth of the description, while the encoding schemes provide
          precision for searching. Since support for precise search counts as 25% of the
          score, a record that makes no use of encoding schemes can score no higher than 4 stars. </p>
         <p>The element part of the score consists of 6 points awarded for each of six basic
         metadata elements that must be present to give the record minimal breadth of
         coverage, plus a further 1.5 points awarded for additional elements that add to the 
         depth of description. These components of the score are awarded as follows:</p>
         <dl>
            <dt>Title</dt>
            <dd>
               <p>1 point is awarded for the presence of a Tittle element. Absence of a
                  title that is inherent to the resource does not block achieving this
                  point, since in that case it is recommended best practice for the
                  cataloger to supply a descriptive title enclosed in square brackets.
                  </p>
            </dd>
            <dt>Date</dt>
            <dd>
               <p>1 point is awarded for the presence of at least one
                  Date element (or any of its refinements). Absence of a
                  date in the resource itself does not block achieving this
                  point, since in that case it is recommended best practice for the
                  cataloger to estimate the date.</p>
            </dd>
            <dt>Agent</dt>
            <dd>
               <p>1 point is awarded for the presence of at least one element that
                  provides an indication of who is behind the resource, whether as
                  Contributor or Creator or Publisher.</p>
            </dd>
            <dt>About</dt>
            <dd>
               <p>1 point is awarded for the presence of at least one element that
                  provides an indication of what the resource is about, whether Subject or
                  Description or Coverage (or any refinement of the latter two).</p>
            </dd>
            <dt>Language</dt>
            <dd>
               <p>1 point is awarded for the presence of at least one Language element.
                  Absence of any natural language content in a resource (such as in a
                  software tool) does not block achieving this
                  point, since in that case it is recommended best practice is to use the
                  ISO 639-3 code <tt>[zxx]</tt> meaning "No linguistic content."  </p>
            </dd>
            <dt>Type</dt>
            <dd><p>1 point is awarded for the presence of at least one Type element.</p></dd>
            <dt>Depth</dt>
            <dd><p>0.25 points up to a maximum of 1.5 points are awarded for each element
               that is present in addition to the ones counted above. This means that in
               order to get the full complement of 7.5 points for the element part of the
               score, the record must have at least 12 elements. This may include elements
            that are not mentioned above as well as multiple instances of those mentioned
            above.</p></dd>
         </dl>
         <p>The encoding scheme part of the score consists of up to 2.5 points awarded as
            follows:</p>
         <dl>
            <dt>ISO639-3</dt>
            <dd>
               <p>0.5 points up to a maximum of 1 point are awarded for each use of
                  the ISO639-3 encoding scheme with the Language element or the Subject 
                  element to precisely identify a language. This encoding scheme is
                  singled out for special weighting since discovery by language is such a
                  fundamental requirement for our community. Every resource can achieve at
                  least 0.5 points with a Language element.  A resource that is not about
                  any language or does not have content in a second language will not be
                  able to achieve the other 0.5 points. This gives a theoretical maximum
                  score of 9.5 points (which still summarizes as 5 stars) for that subset of resources.
               </p>
            </dd>
            <dt>Precision</dt>
            <dd>
               <p>0.25 points up to a maximum of 1.5 points are awarded for each use of an
                  encoding scheme other than ISO639-3.
                  This means that in
                  order to get the full complement of 2.5 points for the schemes part of the
                  score, the record must use an encoding scheme with at least 8 elements.
                  Since encoding schemes are possible with 5 of the 6 mandatory elements,
                  this is typically not difficult to achieve. </p>
            </dd>
         </dl>
       <p>The free-standing metadata service <cit>OLAC-Free</cit> can be used to see what
          quality score will be awarded to a given OLAC metadata record. The XML encoding
          of a record is pasted into a submission form; the service then validates the
          record. If it is valid, a report of the quality score is generated with comments on what must
       be done to raise the score to 10. The same quality analysis is shown for a sample
       record from each participating archive by following the "Sample Record" link on the
       <cit>OLAC-Archives</cit> page.
       </p>
       <p>The average quality score for all the records provided by a given participating
          archive can be seen by following the "Metrics" link on the
          <cit>OLAC-Archives</cit> page. The metrics report also shows the breakdown across the
          collection of all the components that go into the quality score.</p>
      </section>
      <section>
         <heading>Other metrics</heading>
         <p>Could explain other metrics here.</p>
         <p>And if we introduce deductions for citations issued by peer reviewers, that
            would need to be explained as a deduction to the average quality score -- e.g.
            one ponnt per citation..</p>
      </section>


   </body>

   <references>
      <ref abbrev="OLAC-Archives">OLAC: Participating Archives.<br/>
         &lt;<url>http://www.language-archives.org/archives.php4</url>&gt;</ref>
      <ref abbrev="OLAC-BPR">Best Practice Recommendations for Language Resource Description.<br/>
         &lt;<url>http://www.language-archives.org/REC/bpr.html</url>&gt;</ref>
      <ref abbrev="OLAC-Free">Free-standing OLAC Metadata.<br/>
         &lt;<url>http://www.language-archives.org/tools/metadata/freestanding.html</url>&gt;</ref>
      <ref abbrev="OLAC-Process">OLAC Process, Section 2, "Governing ideas".<br/>
         &lt;<url>http://www.language-archives.org/OLAC/process.html#Governing%20ideas</url>&gt;</ref>
      <ref abbrev="OLAC-Usage">OLAC Metadata Usage Guidelines.<br/>
         &lt;<url>http://www.language-archives.org/NOTE/usage.html</url>&gt;</ref>


   </references>

</OLAC_doc>
